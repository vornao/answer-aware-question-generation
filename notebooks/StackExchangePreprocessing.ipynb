{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries for data preprocessing\n",
    "\n",
    "What we want to do is go from HTML formato to .md type of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re \n",
    "import numpy as np\n",
    "from markdownify import markdownify as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying only with ai.stackexchange.com\n",
    "\n",
    "# dataset name is train-00000-of-00001.parquet is in the same folder locally\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=\"../data_preparation/testing_data/train-00000-of-00001.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['qid', 'question', 'answers', 'date', 'metadata'],\n",
      "        num_rows: 2204\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(dataset)\n",
    "\n",
    "# print only 'question' column first 5 rows\n",
    "#print(dataset['train']['question'][:5])\n",
    "\n",
    "#select a split\n",
    "ds = dataset['train']\n",
    "HTML(ds[0]['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing steps\n",
    "\n",
    "We want to take the dataset in HTML from stack exchange and get it in a markdown format, in order to do this we have to:\n",
    "\n",
    "1. Parse the HTML formato to plain text\n",
    "2. Get pair question (good,bad) or (choosen, not choosen)\n",
    "3. One question is composed as follows: qid, question, answers (a list), date, metadata: what we need is inside each answers containing  info about author and score of the question and flagged true or false if selected or not. Idea: coupling the best with the worst or just all without repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def markdowning(html_text):\n",
    "\n",
    "    md_text = md(html_text)\n",
    "    md_text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", md_text).strip()\n",
    "    \n",
    "    return md_text\n",
    "\n",
    "markdowning(ds[0]['question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?\n",
      "==========================================================================================================================================================================================================================================================\n",
      "Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?\n",
      "==========================================================================================================================================================================================================================================================\n",
      "When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?\n",
      "==========================================================================================================================================================================================================================================================\n",
      "Given the following definition of an intelligent agent (taken from a [Wikipedia article](http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition))\n",
      "\n",
      "> \n",
      "> If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent\n",
      "> \n",
      "> \n",
      "> \n",
      "\n",
      "and given that we, humans, all make mistakes, which means that we are not maximizing the expected value of a performance measure, then does this imply that humans are not intelligent?\n",
      "==========================================================================================================================================================================================================================================================\n",
      "This [quote by Stephen Hawking](https://www.independent.co.uk/life-style/gadgets-and-tech/news/stephen-hawking-artificial-intelligence-could-wipe-out-humanity-when-it-gets-too-clever-humans-could-become-ants-being-stepped-a6686496.html) has been in headlines for quite some time:\n",
      "\n",
      "> \n",
      "> Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.\n",
      "> \n",
      "> \n",
      "> \n",
      "\n",
      "Why does he say this? To put it simply: what are the possible threats from AI (that Stephen Hawking is worried about)? If we know that AI is so dangerous, why are we still promoting it? Why is it not banned?\n",
      "\n",
      "What are the adverse consequences of the so-called [Technological Singularity](https://en.wikipedia.org/wiki/Technological_singularity)?\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'question': '<p>What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?</p>\\n',\n",
       " 'answers': [{'answer_id': 3,\n",
       "   'author': 'Franck Dernoncourt',\n",
       "   'author_id': 4,\n",
       "   'author_profile': 'https://ai.stackexchange.com/users/4',\n",
       "   'pm_score': 5,\n",
       "   'selected': True,\n",
       "   'text': '<p>\"Backprop\" is the same as \"backpropagation\": it\\'s just a shorter way to say it. It is sometimes abbreviated as \"BP\".</p>\\n'},\n",
       "  {'answer_id': 83,\n",
       "   'author': 'Dawny33',\n",
       "   'author_id': 101,\n",
       "   'author_profile': 'https://ai.stackexchange.com/users/101',\n",
       "   'pm_score': 2,\n",
       "   'selected': False,\n",
       "   'text': '<p>Yes, as Franck has rightly put, \"backprop\" means backpropogation, which is frequently used in the domain of neural networks for error optimization.</p>\\n\\n<p>For a detailed explanation, I would point out <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"nofollow\">this tutorial</a> on the concept of backpropogation by a very good book of Michael Nielsen. </p>\\n'},\n",
       "  {'answer_id': 222,\n",
       "   'author': 'kenorb',\n",
       "   'author_id': 8,\n",
       "   'author_profile': 'https://ai.stackexchange.com/users/8',\n",
       "   'pm_score': 3,\n",
       "   'selected': False,\n",
       "   'text': '<p>\\'Backprop\\' is short for \\'backpropagation of error\\' in order to avoid confusion when using <em>backpropagation</em> term.</p>\\n\\n<p>Basically <em>backpropagation</em> refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward network<sup>Werbos</sup>. And <em>backprop</em> refers to a training method that uses backpropagation to compute the gradient.</p>\\n\\n<p>So we can say that a <em>backprop</em> network is a feedforward network trained by <em>backpropagation</em>.</p>\\n\\n<p>The \\'standard backprop\\' term is a euphemism for the <em>generalized delta rule</em> which is most widely used supervised training method.</p>\\n\\n<p>Source: <a href=\"ftp://ftp.sas.com/pub/neural/FAQ2.html#A_backprop\" rel=\"noreferrer\">What is backprop?</a> at FAQ of Usenet newsgroup comp.ai.neural-nets</p>\\n\\n<p>References:</p>\\n\\n<ul>\\n<li>Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.</li>\\n<li>Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.</li>\\n<li>Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.</li>\\n<li>Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.</li>\\n<li>Polyak, B.T. (1964), \"Some methods of speeding up the convergence of iteration methods,\" Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.</li>\\n<li>Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.</li>\\n<li>Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.</li>\\n<li>Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), \"Learning internal representations by error propagation\", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.</li>\\n<li>Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley &amp; Sons. Includes Werbos\\'s 1974 Harvard Ph.D. thesis, Beyond Regression.</li>\\n</ul>\\n'},\n",
       "  {'answer_id': 20534,\n",
       "   'author': 'FourierFlux',\n",
       "   'author_id': 32390,\n",
       "   'author_profile': 'https://ai.stackexchange.com/users/32390',\n",
       "   'pm_score': 1,\n",
       "   'selected': False,\n",
       "   'text': \"<p>It's a fancy name for the multivariable chain rule.</p>\\n\"},\n",
       "  {'answer_id': 28584,\n",
       "   'author': 'hanugm',\n",
       "   'author_id': 18758,\n",
       "   'author_profile': 'https://ai.stackexchange.com/users/18758',\n",
       "   'pm_score': 0,\n",
       "   'selected': False,\n",
       "   'text': '<p>We need to compute the gradients in-order to train the deep neural networks. Deep neural network consists of many layers. Weight parameters are present between the layers. Since we need to compute the gradients of loss function for each weight, we use an algorithm called backprop. It is an abbreviation for <strong>backprop</strong>agation, which is also called as error backpropagation or reverse differentiation.</p>\\n<p>It can be understood well from the following paragraph taken from <a href=\"https://web.stanford.edu/%7Ejurafsky/slp3/7.pdf\" rel=\"nofollow noreferrer\">Neural Networks and Neural Language Models</a></p>\\n<blockquote>\\n<p>For deep networks, computing the gradients for each weight is much\\nmore complex,since we are computing the derivative with respect to\\nweight parameters that appear all the way back in the very early\\nlayers of the network, even though the loss is computed only at the\\nvery end of the network.<strong>The solution to computing this gradient is an\\nalgorithm called error backpropagation or backprop</strong>. While backprop was\\ninvented for neural networks, it turns out to be the same as a more\\ngeneral procedure called backward  differentiation, which depends on\\nthe  notion  of computation graphs.</p>\\n</blockquote>\\n'}],\n",
       " 'date': '2016/08/02',\n",
       " 'metadata': ['https://ai.stackexchange.com/questions/1',\n",
       "  'https://ai.stackexchange.com',\n",
       "  'https://ai.stackexchange.com/users/8/']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(markdowning(ds[i]['question']))\n",
    "    print(\"=\"*250)\n",
    "\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(ds):\n",
    "    #TODO We have to define the splits on the big dataset later: finetune, eval, reward model, rl\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking best and worst response for each question in couples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_worst_comparison(answers):\n",
    "\n",
    "    pairs = []\n",
    "    best = False\n",
    "\n",
    "    for answer in answers:\n",
    "        best = answers[0]\n",
    "        worst = answers[-1]\n",
    "        pairs.append((best, worst))\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'answer_id': 3, 'author': 'Franck Dernoncourt', 'author_id': 4, 'author_profile': 'https://ai.stackexchange.com/users/4', 'pm_score': 5, 'selected': True, 'text': '<p>\"Backprop\" is the same as \"backpropagation\": it\\'s just a shorter way to say it. It is sometimes abbreviated as \"BP\".</p>\\n'}, {'answer_id': 83, 'author': 'Dawny33', 'author_id': 101, 'author_profile': 'https://ai.stackexchange.com/users/101', 'pm_score': 2, 'selected': False, 'text': '<p>Yes, as Franck has rightly put, \"backprop\" means backpropogation, which is frequently used in the domain of neural networks for error optimization.</p>\\n\\n<p>For a detailed explanation, I would point out <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" rel=\"nofollow\">this tutorial</a> on the concept of backpropogation by a very good book of Michael Nielsen. </p>\\n'}, {'answer_id': 222, 'author': 'kenorb', 'author_id': 8, 'author_profile': 'https://ai.stackexchange.com/users/8', 'pm_score': 3, 'selected': False, 'text': '<p>\\'Backprop\\' is short for \\'backpropagation of error\\' in order to avoid confusion when using <em>backpropagation</em> term.</p>\\n\\n<p>Basically <em>backpropagation</em> refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward network<sup>Werbos</sup>. And <em>backprop</em> refers to a training method that uses backpropagation to compute the gradient.</p>\\n\\n<p>So we can say that a <em>backprop</em> network is a feedforward network trained by <em>backpropagation</em>.</p>\\n\\n<p>The \\'standard backprop\\' term is a euphemism for the <em>generalized delta rule</em> which is most widely used supervised training method.</p>\\n\\n<p>Source: <a href=\"ftp://ftp.sas.com/pub/neural/FAQ2.html#A_backprop\" rel=\"noreferrer\">What is backprop?</a> at FAQ of Usenet newsgroup comp.ai.neural-nets</p>\\n\\n<p>References:</p>\\n\\n<ul>\\n<li>Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.</li>\\n<li>Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.</li>\\n<li>Bertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.</li>\\n<li>Bertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.</li>\\n<li>Polyak, B.T. (1964), \"Some methods of speeding up the convergence of iteration methods,\" Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.</li>\\n<li>Polyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.</li>\\n<li>Reed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.</li>\\n<li>Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), \"Learning internal representations by error propagation\", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.</li>\\n<li>Werbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley &amp; Sons. Includes Werbos\\'s 1974 Harvard Ph.D. thesis, Beyond Regression.</li>\\n</ul>\\n'}, {'answer_id': 20534, 'author': 'FourierFlux', 'author_id': 32390, 'author_profile': 'https://ai.stackexchange.com/users/32390', 'pm_score': 1, 'selected': False, 'text': \"<p>It's a fancy name for the multivariable chain rule.</p>\\n\"}, {'answer_id': 28584, 'author': 'hanugm', 'author_id': 18758, 'author_profile': 'https://ai.stackexchange.com/users/18758', 'pm_score': 0, 'selected': False, 'text': '<p>We need to compute the gradients in-order to train the deep neural networks. Deep neural network consists of many layers. Weight parameters are present between the layers. Since we need to compute the gradients of loss function for each weight, we use an algorithm called backprop. It is an abbreviation for <strong>backprop</strong>agation, which is also called as error backpropagation or reverse differentiation.</p>\\n<p>It can be understood well from the following paragraph taken from <a href=\"https://web.stanford.edu/%7Ejurafsky/slp3/7.pdf\" rel=\"nofollow noreferrer\">Neural Networks and Neural Language Models</a></p>\\n<blockquote>\\n<p>For deep networks, computing the gradients for each weight is much\\nmore complex,since we are computing the derivative with respect to\\nweight parameters that appear all the way back in the very early\\nlayers of the network, even though the loss is computed only at the\\nvery end of the network.<strong>The solution to computing this gradient is an\\nalgorithm called error backpropagation or backprop</strong>. While backprop was\\ninvented for neural networks, it turns out to be the same as a more\\ngeneral procedure called backward  differentiation, which depends on\\nthe  notion  of computation graphs.</p>\\n</blockquote>\\n'}], [{'answer_id': 18, 'author': 'wythagoras', 'author_id': 29, 'author_profile': 'https://ai.stackexchange.com/users/29', 'pm_score': 2, 'selected': False, 'text': \"<blockquote>\\n  <p>To put it simply in layman terms, what are the possible threats from AI? </p>\\n</blockquote>\\n\\n<p>Currently, there are no threat. </p>\\n\\n<p>The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity).  However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are.</p>\\n\\n<p>This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like. </p>\\n\\n<blockquote>\\n  <p>If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>\\n</blockquote>\\n\\n<p>As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us. </p>\\n\"}, {'answer_id': 19, 'author': 'dorien', 'author_id': 52, 'author_profile': 'https://ai.stackexchange.com/users/52', 'pm_score': 2, 'selected': False, 'text': '<p>Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.</p>\\n'}, {'answer_id': 22, 'author': 'mindcrime', 'author_id': 33, 'author_profile': 'https://ai.stackexchange.com/users/33', 'pm_score': 3, 'selected': False, 'text': '<p>It\\'s not just Hawking, you hear variations on this refrain from a lot of people.  And given that they\\'re mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn\\'t be dismissed out of hand.</p>\\n\\n<p>Anyway, the basic idea seems to be this: If we create \"real\" artificial intelligence, at some point, it will be able to improve itself, which improves it\\'s ability to improve itself, which means it can improve it\\'s ability to improve itself even more, and so on... a runaway cascade leading to \"superhuman intelligence\".  That is to say, leading to something that more intelligent than we area.</p>\\n\\n<p>So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us?  Well, it certainly seems reasonable to speculate that it <em>could</em> be so.   OTOH, we have no particular reason, right now, to think that it <em>will</em> be so. </p>\\n\\n<p>So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things.  Since we don\\'t <em>know</em> if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it\\'s smarter than we are!), it\\'s a reasonable thing to take under consideration.</p>\\n\\n<p>Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous \"AI Box\" experiment.  I think anybody interested in this topic should read some of his material.</p>\\n\\n<p><a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"noreferrer\">http://www.yudkowsky.net/singularity/aibox/</a></p>\\n'}, {'answer_id': 23, 'author': 'Franck Dernoncourt', 'author_id': 4, 'author_profile': 'https://ai.stackexchange.com/users/4', 'pm_score': 2, 'selected': False, 'text': '<p>As Andrew Ng <a href=\"http://www.theregister.co.uk/2015/03/19/andrew_ng_baidu_ai/\" rel=\"nofollow noreferrer\">said</a>, worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. </p>\\n\\n<p><a href=\"https://i.stack.imgur.com/m6jnl.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/m6jnl.png\" alt=\"enter image description here\"></a></p>\\n\\n<p>That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see <a href=\"https://en.wikipedia.org/wiki/Roboethics\" rel=\"nofollow noreferrer\">Roboethics</a>.</p>\\n'}, {'answer_id': 24, 'author': 'zavtra', 'author_id': 56, 'author_profile': 'https://ai.stackexchange.com/users/56', 'pm_score': 2, 'selected': False, 'text': \"<p>He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed.</p>\\n\\n<p>We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless.</p>\\n\\n<p>The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income?</p>\\n\\n<p>The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please.</p>\\n\\n<p>But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt.</p>\\n\"}, {'answer_id': 25, 'author': 'Matthew Gray', 'author_id': 10, 'author_profile': 'https://ai.stackexchange.com/users/10', 'pm_score': 2, 'selected': False, 'text': '<p>There are a number of long resources to answer this sort of question: consider Stuart Armstrong\\'s book <a href=\"http://rads.stackoverflow.com/amzn/click/B00IB4N4KU\" rel=\"nofollow\">Smarter Than Us</a>, Nick Bostrom\\'s book <a href=\"http://rads.stackoverflow.com/amzn/click/B00LOOCGB2\" rel=\"nofollow\">Superintelligence</a>, which grew out of this <a href=\"http://www.nickbostrom.com/views/superintelligence.pdf\" rel=\"nofollow\">edge.org answer</a>, <a href=\"http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\" rel=\"nofollow\">Tim Urban\\'s explanation</a>, or <a href=\"https://aisafety.wordpress.com/\" rel=\"nofollow\">Michael Cohen\\'s explanation</a>.</p>\\n\\n<p>But here\\'s my (somewhat shorter) answer: intelligence is all about decision-making, and we don\\'t have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.)</p>\\n\\n<p>This isn\\'t necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn\\'t share our values.</p>\\n'}])\n"
     ]
    }
   ],
   "source": [
    "pairs = best_worst_comparison(ds[0:5]['answers'])\n",
    "\n",
    "\n",
    "# print the first pair\n",
    "\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
