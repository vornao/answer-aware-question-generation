{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagenfs/l.miglior/answer-aware-question-generation/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-28 17:46:48,266] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagenfs/l.miglior/answer-aware-question-generation/.venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "\"\"\"\n",
    "Train fine tuned T5 model with Proximal Policy Optimization (PPO) algorithm.\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--model_name\", type=str, default=\"t5-small\")\n",
    "#parser.add_argument(\"--highlight\", type=bool, default=True)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "average_question_length = 10.0\n",
    "\n",
    "HIGHLIGHT = True\n",
    "TOKEN_QUESTION = \"<question>\"\n",
    "TOKEN_END_QUESTION = \"<question>\"\n",
    "TOKEN_CONTEXT = \"<context>\"\n",
    "TOKEN_END_CONTEXT = \"<context>\"\n",
    "TOKEN_ANSWER = \"<answer>\"\n",
    "TOKEN_END_ANSWER = \"<answer>\"\n",
    "HIGHLIGHT_ANSWER = \"<hl>\"\n",
    "SPLIT_SEED = 42\n",
    "NPROC = 32\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "HIGHLIGHT = True\n",
    "if HIGHLIGHT:\n",
    "    model_name = f\"{model_name}-hl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "WARNING:root:`peft_config` argument ignored since a peft config file was found in ./models/t5-small-hl/\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForSeq2SeqLM'> model is loaded from './models/t5-small-hl/', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForSeq2SeqLM'> model is loaded from './models/t5-small-hl/', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    f\"./models/{model_name}/\", device_map=\"auto\", load_in_8bit=True, peft_config=peft_config\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"./models/{model_name}\", model_max_length=512)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_target(e):\n",
    "    answer_start = e[\"answers\"][\"answer_start\"][0]\n",
    "    # add highlight token to context\n",
    "    ans_len = len(e[\"answers\"][\"text\"][0])\n",
    "\n",
    "    if HIGHLIGHT:\n",
    "        e[\"context\"] = (\n",
    "            e[\"context\"][:answer_start]\n",
    "            + \" \"\n",
    "            + HIGHLIGHT_ANSWER\n",
    "            + \" \"\n",
    "            + e[\"context\"][answer_start : answer_start + ans_len]\n",
    "            + \" \"\n",
    "            + HIGHLIGHT_ANSWER\n",
    "            + \" \"\n",
    "            + e[\"context\"][answer_start + ans_len :]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        # answer + context\n",
    "        \"inputs\": f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}',\n",
    "        # question\n",
    "        \"target\": f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}',\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_squad_dataset(dataset_name=\"squad\", split=\"train\"):\n",
    "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
    "    # Add question, answer and context tokens to dataset in a new column named text\n",
    "    dataset = dataset.map(\n",
    "        lambda e: {\n",
    "            # answer + context\n",
    "            # changed to 'query' for PPO\n",
    "            \"query\": f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}',\n",
    "            # question\n",
    "            \"target\": f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}',\n",
    "        },\n",
    "        num_proc=NPROC,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Need to have training dataset aligned with PPO input format\n",
    "\n",
    "train_dataset = preprocess_squad_dataset(dataset_name=\"squad\", split=\"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).squeeze()\n",
    "    return sample\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_dataset = train_dataset.map(tokenize, num_proc=NPROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    log_with='tensorboard',\n",
    "    project_kwargs={'logging_dir': f'./logs/{model_name}'},\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/trl/main/en/how_to_train#how-to-generate-text-for-training\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_model(prediction, example):\n",
    "    \"\"\"\n",
    "    this function will return a reward function for PPO\n",
    "    \"\"\"\n",
    "\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    reward = bertscore.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[question],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "    )[\"f1\"].item()\n",
    "\n",
    "    repetition_penalty = -1.0 if answer.lower() in prediction.lower() else 1.0\n",
    "    question_word_penalty = -0.5 if question.split()[0].lower() != prediction.split()[0].lower() else 0.5\n",
    "    ans_in_question_penalty = -1.0 if answer.lower() in question.lower() else 1.0\n",
    "    question_length_penalty = -0.5 if len(question.split()) > average_question_length else 0.5\n",
    "\n",
    "    reward += repetition_penalty + question_word_penalty + ans_in_question_penalty + question_length_penalty\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/storagenfs/l.miglior/answer-aware-question-generation/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/87599 [02:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/storagenfs/l.miglior/answer-aware-question-generation/scripts/t5-ppo.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhlt/storagenfs/l.miglior/answer-aware-question-generation/scripts/t5-ppo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(ppo_trainer\u001b[39m.\u001b[39mdataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhlt/storagenfs/l.miglior/answer-aware-question-generation/scripts/t5-ppo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     query_tensors \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhlt/storagenfs/l.miglior/answer-aware-question-generation/scripts/t5-ppo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     response_tensors \u001b[39m=\u001b[39m ppo_trainer\u001b[39m.\u001b[39;49mgenerate(query_tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhlt/storagenfs/l.miglior/answer-aware-question-generation/scripts/t5-ppo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     batch[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(r\u001b[39m.\u001b[39msqueeze()) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response_tensors]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhlt/storagenfs/l.miglior/answer-aware-question-generation/scripts/t5-ppo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:459\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, length_sampler, batch_size, return_prompt, generate_ref_response, **generation_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     ref_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_peft_model \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mref_model\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(query_tensor, List):\n\u001b[0;32m--> 459\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_batched(\n\u001b[1;32m    460\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    461\u001b[0m         query_tensor,\n\u001b[1;32m    462\u001b[0m         length_sampler\u001b[39m=\u001b[39;49mlength_sampler,\n\u001b[1;32m    463\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    464\u001b[0m         return_prompt\u001b[39m=\u001b[39;49mreturn_prompt,\n\u001b[1;32m    465\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m     \u001b[39mif\u001b[39;00m generate_ref_response:\n\u001b[1;32m    468\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptional_peft_ctx():\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:541\u001b[0m, in \u001b[0;36mPPOTrainer._generate_batched\u001b[0;34m(self, model, query_tensors, length_sampler, batch_size, return_prompt, pad_to_multiple_of, remove_padding, **generation_kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: batch, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: batch_mask}\n\u001b[1;32m    533\u001b[0m padded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m     inputs,\n\u001b[1;32m    535\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    539\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_device)\n\u001b[0;32m--> 541\u001b[0m generations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49munwrap_model(model)\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpadded_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs)\n\u001b[1;32m    543\u001b[0m \u001b[39mfor\u001b[39;00m generation, mask \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(generations, padded_inputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/trl/models/modeling_value_head.py:429\u001b[0m, in \u001b[0;36mAutoModelForSeq2SeqLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    426\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    We call `generate` on the wrapped model.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/peft/peft_model.py:1277\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m peft_config\u001b[39m.\u001b[39mis_prompt_learning:\n\u001b[0;32m-> 1277\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1278\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1720\u001b[0m         input_ids,\n\u001b[1;32m   1721\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1722\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1723\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1724\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1725\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1726\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1727\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1728\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1729\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1733\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2837\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[39m# sample\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(next_token_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 2837\u001b[0m next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m   2839\u001b[0m \u001b[39m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2840\u001b[0m \u001b[39mif\u001b[39;00m eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in tqdm(ppo_trainer.dataloader):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "    batch[\"prediction\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    print(batch[\"prediction\"])\n",
    "    pipe_outputs = reward_model(*zip(batch[\"query\"], batch[\"prediction\"]))\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
