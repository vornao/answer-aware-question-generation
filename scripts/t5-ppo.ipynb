{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "\"\"\"\n",
    "Train fine tuned T5 model with Proximal Policy Optimization (PPO) algorithm.\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--model_name\", type=str, default=\"t5-small\")\n",
    "#parser.add_argument(\"--highlight\", type=bool, default=True)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "average_question_length = 10.0\n",
    "\n",
    "HIGHLIGHT = True\n",
    "TOKEN_QUESTION = \"<question>\"\n",
    "TOKEN_END_QUESTION = \"<question>\"\n",
    "TOKEN_CONTEXT = \"<context>\"\n",
    "TOKEN_END_CONTEXT = \"<context>\"\n",
    "TOKEN_ANSWER = \"<answer>\"\n",
    "TOKEN_END_ANSWER = \"<answer>\"\n",
    "HIGHLIGHT_ANSWER = \"<hl>\"\n",
    "SPLIT_SEED = 42\n",
    "NPROC = 32\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "HIGHLIGHT = True\n",
    "if HIGHLIGHT:\n",
    "    model_name = f\"{model_name}-hl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'peft.peft_model.PeftModelForSeq2SeqLM'> model is loaded from './models/t5-base-hl/', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForSeq2SeqLM'> model is loaded from './models/t5-base-hl/', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AutoModelForSeq2SeqLMWithValueHead' object has no attribute '_prepare_encoder_decoder_kwargs_for_generation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.11/site-packages/peft/tuners/lora.py:492\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute '_prepare_encoder_decoder_kwargs_for_generation'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# from here we can try this:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# https://ankushmulkar.medium.com/finetune-flan-t5-using-reinforcement-learning-ppo-and-peft-for-producing-non-toxic-summaries-fed2695ee4f4\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Alternatively, we can use the following:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLMWithValueHead\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./models/t5-base-hl/\u001b[39m\u001b[39m\"\u001b[39m, device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, load_in_8bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m peft_model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39;49mfrom_pretrained(model,\u001b[39m\"\u001b[39;49m\u001b[39m./models/t5-small-hl/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                                        lora_config\u001b[39m=\u001b[39;49mpeft_config, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m                                        device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,                                       \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m                                        is_trainable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/v.gargano1/answer-aware-question-generation/scripts/t5-ppo.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPEFT model parameters to be updated:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mpeft_model\u001b[39m.\u001b[39mprint_trainable_parameters()\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.11/site-packages/peft/peft_model.py:277\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(model, config, adapter_name)\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     model \u001b[39m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[39m.\u001b[39;49mtask_type](model, config, adapter_name)\n\u001b[1;32m    278\u001b[0m model\u001b[39m.\u001b[39mload_adapter(model_id, adapter_name, is_trainable\u001b[39m=\u001b[39mis_trainable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.11/site-packages/peft/peft_model.py:1059\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(model, peft_config, adapter_name)\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation\n\u001b[1;32m   1058\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_encoder_decoder_kwargs_for_generation \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1059\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation\n\u001b[1;32m   1060\u001b[0m )\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.11/site-packages/peft/tuners/lora.py:494\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, name)\n",
      "File \u001b[0;32m~/answer-aware-question-generation/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoModelForSeq2SeqLMWithValueHead' object has no attribute '_prepare_encoder_decoder_kwargs_for_generation'"
     ]
    }
   ],
   "source": [
    "# from here we can try this:\n",
    "# https://ankushmulkar.medium.com/finetune-flan-t5-using-reinforcement-learning-ppo-and-peft-for-producing-non-toxic-summaries-fed2695ee4f4\n",
    "\n",
    "# Alternatively, we can use the following:\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(f\"./models/t5-base-hl/\", device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\"./models/t5-small-hl/\",\n",
    "                                       lora_config=peft_config, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{peft_model.print_trainable_parameters()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second method failing: Use PeftModel.from_pretrained and then AutoModelForSeq2SeqLMWithValueHead.from_pretrained\n",
    "#peft_model = PeftModel.from_pretrained(f\"./models/{model_name}\", device_map=\"auto\", peft_config=peft_config, is_trainable=True, model_id=\"./models/t5-small-hl/\")\n",
    "\n",
    "\n",
    "\n",
    "# First method failing : \n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(f\"./models/{model_name}\", device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"./models/{model_name}\", model_max_length=512)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model given mode_name\n",
    "#model = transformers.T5ForConditionalGeneration.from_pretrained(f\"./models/{model_name}\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(f\"./models/{model_name}\", device_map=\"auto\", load_in_8bit=True, peft_config=peft_config)\n",
    "\n",
    "# Get peft model\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"./models/{model_name}\", model_max_length=512)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    f\"./models/{model_name}/\", device_map=\"auto\", load_in_8bit=True, peft_config=peft_config\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"./models/{model_name}\", model_max_length=512)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_target(e):\n",
    "    answer_start = e[\"answers\"][\"answer_start\"][0]\n",
    "    # add highlight token to context\n",
    "    ans_len = len(e[\"answers\"][\"text\"][0])\n",
    "\n",
    "    if HIGHLIGHT:\n",
    "        e[\"context\"] = (\n",
    "            e[\"context\"][:answer_start]\n",
    "            + \" \"\n",
    "            + HIGHLIGHT_ANSWER\n",
    "            + \" \"\n",
    "            + e[\"context\"][answer_start : answer_start + ans_len]\n",
    "            + \" \"\n",
    "            + HIGHLIGHT_ANSWER\n",
    "            + \" \"\n",
    "            + e[\"context\"][answer_start + ans_len :]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        # answer + context\n",
    "        \"inputs\": f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}',\n",
    "        # question\n",
    "        \"target\": f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}',\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_squad_dataset(dataset_name=\"squad\", split=\"train\"):\n",
    "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
    "    # Add question, answer and context tokens to dataset in a new column named text\n",
    "    dataset = dataset.map(\n",
    "        lambda e: {\n",
    "            # answer + context\n",
    "            # changed to 'query' for PPO\n",
    "            \"query\": f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}',\n",
    "            # question\n",
    "            \"target\": f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}',\n",
    "        },\n",
    "        num_proc=NPROC,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Need to have training dataset aligned with PPO input format\n",
    "\n",
    "train_dataset = preprocess_squad_dataset(dataset_name=\"squad\", split=\"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).squeeze()\n",
    "    return sample\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_dataset = train_dataset.map(tokenize, num_proc=NPROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    log_with='tensorboard',\n",
    "    project_kwargs={'logging_dir': f'./logs/{model_name}'},\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/trl/main/en/how_to_train#how-to-generate-text-for-training\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_model(prediction, example):\n",
    "    \"\"\"\n",
    "    this function will return a reward function for PPO\n",
    "    \"\"\"\n",
    "\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    reward = bertscore.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[question],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "    )[\"f1\"].item()\n",
    "\n",
    "    repetition_penalty = -1.0 if answer.lower() in prediction.lower() else 1.0\n",
    "    question_word_penalty = -0.5 if question.split()[0].lower() != prediction.split()[0].lower() else 0.5\n",
    "    ans_in_question_penalty = -1.0 if answer.lower() in question.lower() else 1.0\n",
    "    question_length_penalty = -0.5 if len(question.split()) > average_question_length else 0.5\n",
    "\n",
    "    reward += repetition_penalty + question_word_penalty + ans_in_question_penalty + question_length_penalty\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(ppo_trainer.dataloader):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "    batch[\"prediction\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    print(batch[\"prediction\"])\n",
    "    pipe_outputs = reward_model(*zip(batch[\"query\"], batch[\"prediction\"]))\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
