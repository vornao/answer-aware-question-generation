{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagenfs/l.miglior/answer-aware-question-generation/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-29 10:32:24,681] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagenfs/l.miglior/answer-aware-question-generation/.venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModelForSeq2SeqLM, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\"\"\"\n",
    "Train fine tuned T5 model with Proximal Policy Optimization (PPO) algorithm.\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--model_name\", type=str, default=\"t5-small\")\n",
    "#parser.add_argument(\"--highlight\", type=bool, default=True)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "average_question_length = 10.0\n",
    "\n",
    "HIGHLIGHT = True\n",
    "TOKEN_QUESTION = \"<question>\"\n",
    "TOKEN_END_QUESTION = \"<question>\"\n",
    "TOKEN_CONTEXT = \"<context>\"\n",
    "TOKEN_END_CONTEXT = \"<context>\"\n",
    "TOKEN_ANSWER = \"<answer>\"\n",
    "TOKEN_END_ANSWER = \"<answer>\"\n",
    "HIGHLIGHT_ANSWER = \"<hl>\"\n",
    "SPLIT_SEED = 42\n",
    "NPROC = 32\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "HIGHLIGHT = True\n",
    "if HIGHLIGHT:\n",
    "    model_name = f\"{model_name}-hl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(f\"./models/{model_name}/\")\n",
    "peft_model = PeftModel.from_pretrained(model, model_id=f\"./models/{model_name}/\", config=peft_config, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead(peft_model)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead(model)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"./models/{model_name}\", model_max_length=512)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_target(e):\n",
    "    answer_start = e[\"answers\"][\"answer_start\"][0]\n",
    "    # add highlight token to context\n",
    "    ans_len = len(e[\"answers\"][\"text\"][0])\n",
    "\n",
    "    if HIGHLIGHT:\n",
    "        e[\"context\"] = (\n",
    "            e[\"context\"][:answer_start]\n",
    "            + \" \"\n",
    "            + HIGHLIGHT_ANSWER\n",
    "            + \" \"\n",
    "            + e[\"context\"][answer_start : answer_start + ans_len]\n",
    "            + \" \"\n",
    "            + HIGHLIGHT_ANSWER\n",
    "            + \" \"\n",
    "            + e[\"context\"][answer_start + ans_len :]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        # answer + context\n",
    "        \"inputs\": f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}',\n",
    "        # question\n",
    "        \"target\": f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}',\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_squad_dataset(dataset_name=\"squad\", split=\"train\"):\n",
    "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
    "    # Add question, answer and context tokens to dataset in a new column named text\n",
    "    dataset = dataset.map(\n",
    "        lambda e: {\n",
    "            # answer + context\n",
    "            # changed to 'query' for PPO\n",
    "            \"query\": f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}',\n",
    "            # question\n",
    "            \"target\": f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}',\n",
    "        },\n",
    "        num_proc=NPROC,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Need to have training dataset aligned with PPO input format\n",
    "\n",
    "train_dataset = preprocess_squad_dataset(dataset_name=\"squad\", split=\"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=32): 100%|██████████| 87599/87599 [00:07<00:00, 11379.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"], return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512).squeeze()\n",
    "    return sample\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_dataset = train_dataset.map(tokenize, num_proc=NPROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<question> To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? <question>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.select(range(1))['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_model(example):\n",
    "    \"\"\"\n",
    "    this function will return a reward function for PPO\n",
    "    \"\"\"\n",
    "\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    reward = bertscore.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[question],\n",
    "        lang=\"en\",\n",
    "        model_type=\"bert-base-uncased\",\n",
    "    )[\"f1\"].item()\n",
    "\n",
    "    repetition_penalty = -1.0 if answer.lower() in prediction.lower() else 1.0\n",
    "    question_word_penalty = -0.5 if question.split()[0].lower() != prediction.split()[0].lower() else 0.5\n",
    "    ans_in_question_penalty = -1.0 if answer.lower() in question.lower() else 1.0\n",
    "    question_length_penalty = -0.5 if len(question.split()) > average_question_length else 0.5\n",
    "\n",
    "    reward += repetition_penalty + question_word_penalty + ans_in_question_penalty + question_length_penalty\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    log_with='tensorboard',\n",
    "    project_kwargs={'logging_dir': f'./logs/{model_name}'},\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=ppo_model,\n",
    "    ref_model=ref_model,\n",
    "    config=config,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> s s s s s s s s s '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['prediction'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1368 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(ppo_trainer.dataloader):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    response_tensors = ppo_trainer.generate(query_tensors)\n",
    "    batch[\"prediction\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    print(batch[\"prediction\"])\n",
    "    pipe_outputs = reward_model(*zip(batch[\"query\"], batch[\"prediction\"]))\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
