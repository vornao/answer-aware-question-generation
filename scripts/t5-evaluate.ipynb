{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Notebook\n",
    "> model to evaluate below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model name\n",
    "model_name = \"t5-small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train T5 model on a given dataset, for answer aware question generation task. (Basically a sequence to sequence task)\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "import gc\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "HIGHLIGHT = True\n",
    "TOKEN_QUESTION = '<question>'\n",
    "TOKEN_END_QUESTION = '<question>'\n",
    "TOKEN_CONTEXT = '<context>'\n",
    "TOKEN_END_CONTEXT = '<context>'\n",
    "TOKEN_ANSWER = '<answer>'\n",
    "TOKEN_END_ANSWER = '<answer>'  \n",
    "HIGHLIGHT_ANSWER = '<hl>'\n",
    "SPLIT_SEED = 42\n",
    "NPROC = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.T5ForConditionalGeneration.from_pretrained(f\"./models/{model_name}/\", device_map='auto')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(f\"./models/{model_name}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQuAD preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_target(e):\n",
    "        answer_start = e['answers']['answer_start'][0]\n",
    "        # add highlight token to context\n",
    "        ans_len = len(e['answers']['text'][0])\n",
    "\n",
    "        if HIGHLIGHT:\n",
    "            e[\"context\"] = e[\"context\"][:answer_start] + ' ' + HIGHLIGHT_ANSWER + ' ' +e[\"context\"][answer_start:answer_start+ans_len] + ' ' + HIGHLIGHT_ANSWER +' '+ e[\"context\"][answer_start+ans_len:]\n",
    "        \n",
    "        return {\n",
    "            # answer + context\n",
    "            'inputs': f'generate question: {TOKEN_ANSWER} {e[\"answers\"][\"text\"][0]} {TOKEN_END_ANSWER} {TOKEN_CONTEXT} {e[\"context\"]} {TOKEN_END_CONTEXT}', \n",
    "            # question\n",
    "            'target': f'{TOKEN_QUESTION} {e[\"question\"]} {TOKEN_END_QUESTION}'\n",
    "        }\n",
    "\n",
    "\n",
    "def preprocess_squad_dataset(dataset_name='squad', split='train'):\n",
    "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
    "    # Add question, answer and context tokens to dataset in a new column named text\n",
    "    dataset = dataset.map(get_inputs_target, num_proc=NPROC)\n",
    "    \n",
    "    # Remove unnecessary columns, leaving only the formatted_text column\n",
    "    dataset = dataset.remove_columns(['answers', 'context', 'question'])\n",
    "    return dataset\n",
    "\n",
    "# create a tokenizer function\n",
    "def tokenize_function(example, max_context_length=512, max_question_length=32):\n",
    "# Combine context and question\n",
    "    # Tokenize input (context + answer)\n",
    "    inputs = tokenizer(example['inputs'], max_length=(max_context_length), return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(example['target'], max_length=max_question_length, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"labels\": labels[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=32): 100%|██████████| 87599/87599 [00:02<00:00, 29606.86 examples/s] \n",
      "Map (num_proc=32): 100%|██████████| 10570/10570 [00:01<00:00, 9956.23 examples/s]\n",
      "Map (num_proc=32): 100%|██████████| 87599/87599 [00:05<00:00, 15864.66 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:01<00:00, 778.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = preprocess_squad_dataset(dataset_name='squad', split='train')\n",
    "valid_dataset = preprocess_squad_dataset(dataset_name='squad', split='validation')\n",
    "train, validation = dataset, valid_dataset\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset_train = train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=32,\n",
    "    remove_columns=['inputs', 'target', 'title', 'id'],\n",
    ")\n",
    "\n",
    "tokenized_dataset_validation = validation.select(range(1000)).map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['inputs', 'target', 'title', 'id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generate question: <answer> Saint Bernadette Soubirous <answer> <context> Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to  <hl> Saint Bernadette Soubirous <hl>  in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. <context>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select(range(1))['inputs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/storagenfs/l.miglior/answer-aware-question-generation/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [18:37<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "\n",
    "def generate_question(example, max_length=32):\n",
    "    ids = tokenizer.encode(example, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(input_ids=ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_question_generation(dataset, max_length=32):\n",
    "    for example in tqdm(dataset):\n",
    "        predictions.append(generate_question(example['inputs'], max_length=max_length))\n",
    "\n",
    "\n",
    "dataset = preprocess_squad_dataset(dataset_name='squad', split='validation').select(range(1000, 2000))    \n",
    "evaluate_question_generation(dataset, max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [prediction.replace('question>', '').replace('<question>', '') for prediction in predictions]\n",
    "targets = [target.replace('<question>', '') for target in dataset['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: 0.8905211106538773\n",
      "ROUGE: 0.35782651017737593\n"
     ]
    }
   ],
   "source": [
    "bscore = bertscore.compute(predictions=predictions, references=targets, lang='en')\n",
    "rougescore = rouge.compute(predictions=predictions, references=targets)\n",
    "\n",
    "print(f\"BERTScore: {np.mean(bscore['f1'])}\")\n",
    "print(f\"ROUGE: {rougescore['rouge1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval results\n",
    "```` T5-SMALL SQUAD: 0.89937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'bertscore': bscore,\n",
    "    'rouge': rougescore,\n",
    "    'predictions' : predictions,\n",
    "    'targets' : targets\n",
    "}\n",
    "\n",
    "# save results in a json file into ./models/{model_name}/metrics.json\n",
    "\n",
    "with open(f\"./models/{model_name}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
